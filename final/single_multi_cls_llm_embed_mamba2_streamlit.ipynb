{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jeremy-su1/ai-algorithm/blob/main/final/single_multi_cls_llm_embed_mamba2_streamlit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qdqdxUK_0-h",
        "outputId": "9fe8caa2-604d-4916-e1b5-ec671d483bf0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.6)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec (from torch)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
            "Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.1.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m91.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hLooking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Collecting torch==2.1.1\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torch-2.1.1%2Bcu118-cp310-cp310-linux_x86_64.whl (2325.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 GB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.16.1\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.16.1%2Bcu118-cp310-cp310-linux_x86_64.whl (6.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchaudio==2.1.1\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.1.1%2Bcu118-cp310-cp310-linux_x86_64.whl (3.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m66.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.1.1) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.1.1) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.1.1) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.1.1) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.1) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.1.1) (2024.9.0)\n",
            "Collecting triton==2.1.0 (from torch==2.1.1)\n",
            "  Downloading https://download.pytorch.org/whl/triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.16.1) (1.26.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision==0.16.1) (2.32.3)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.16.1) (11.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.1.1) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.16.1) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.16.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.16.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.16.1) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.1.1) (1.3.0)\n",
            "Installing collected packages: triton, torch, torchvision, torchaudio\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.5.0+cu121\n",
            "    Uninstalling torch-2.5.0+cu121:\n",
            "      Successfully uninstalled torch-2.5.0+cu121\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.20.0+cu121\n",
            "    Uninstalling torchvision-0.20.0+cu121:\n",
            "      Successfully uninstalled torchvision-0.20.0+cu121\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.5.0+cu121\n",
            "    Uninstalling torchaudio-2.5.0+cu121:\n",
            "      Successfully uninstalled torchaudio-2.5.0+cu121\n",
            "Successfully installed torch-2.1.1+cu118 torchaudio-2.1.1+cu118 torchvision-0.16.1+cu118 triton-2.1.0\n",
            "Collecting mamba-ssm\n",
            "  Downloading mamba_ssm-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from mamba-ssm) (2.1.1+cu118)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from mamba-ssm) (24.2)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from mamba-ssm) (1.11.1.1)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from mamba-ssm) (0.8.0)\n",
            "Requirement already satisfied: triton in /usr/local/lib/python3.10/dist-packages (from mamba-ssm) (2.1.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from mamba-ssm) (4.46.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (2024.9.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (1.26.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (0.20.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (4.66.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->mamba-ssm) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->mamba-ssm) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->mamba-ssm) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->mamba-ssm) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->mamba-ssm) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->mamba-ssm) (1.3.0)\n",
            "Building wheels for collected packages: mamba-ssm\n",
            "  Building wheel for mamba-ssm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mamba-ssm: filename=mamba_ssm-2.2.2-cp310-cp310-linux_x86_64.whl size=343412612 sha256=150be68a91b3444b35ee2cda6a4790aaf83711009a8bf8e91afc7a316b091df6\n",
            "  Stored in directory: /root/.cache/pip/wheels/57/7c/90/9f963468ecc3791e36e388f9e7b4a4e1e3f90fbb340055aa4d\n",
            "Successfully built mamba-ssm\n",
            "Installing collected packages: mamba-ssm\n",
            "Successfully installed mamba-ssm-2.2.2\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.1.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.1+cu118)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: evaluate\n",
            "Successfully installed evaluate-0.4.3\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.26.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2024.9.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2024.8.30)\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!pip install torch transformers tqdm datasets\n",
        "!pip install -q streamlit\n",
        "!pip install torch==2.1.1 torchvision==0.16.1 torchaudio==2.1.1 --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install causal-conv1d>=1.1.0\n",
        "!pip install mamba-ssm\n",
        "!pip install datasets evaluate accelerate\n",
        "!pip install huggingface_hub\n",
        "!export LC_ALL=\"en_US.UTF-8\"\n",
        "!export LD_LIBRARY_PATH=\"/usr/lib64-nvidia\"\n",
        "!export LIBRARY_PATH=\"/usr/local/cuda/lib64/stubs\"\n",
        "!ldconfig /usr/lib64-nvidia"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import streamlit as st\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BertTokenizer, BertModel, RobertaTokenizer, RobertaModel\n",
        "from transformers import AutoConfig\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import os\n",
        "import random\n",
        "import json\n",
        "from collections import namedtuple\n",
        "from dataclasses import dataclass, field, asdict\n",
        "from mamba_ssm.models.mixer_seq_simple import MambaLMHeadModel\n",
        "from mamba_ssm.utils.hf import load_config_hf, load_state_dict_hf\n",
        "\n",
        "import evaluate\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from transformers import Trainer\n",
        "from transformers import AutoTokenizer, TrainingArguments\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()\n",
        "\n",
        "from dataclasses import dataclass, asdict\n",
        "import json\n",
        "import streamlit as st\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "\n",
        "# Streamlit 앱의 제목 설정\n",
        "st.title('Classification Fine-tuning App')\n",
        "\n",
        "# 사용자가 선택한 classification 유형\n",
        "classification_type = st.selectbox('Select Classification Type', ['Single-Label Classification', 'Multi-Label Classification'])\n",
        "\n",
        "\n",
        "if classification_type == 'Single-Label Classification':\n",
        "\n",
        "    class DownstreamModel(nn.Module):\n",
        "        def __init__(self, class_num, SIGMA):\n",
        "            super(DownstreamModel, self).__init__()\n",
        "            self.SIGMA = SIGMA\n",
        "            self.compress_layers = nn.ModuleList()\n",
        "            for _ in range(5):\n",
        "                layers = []\n",
        "                layers.append(nn.Linear(2048, 1024))\n",
        "                layers.append(nn.ReLU())\n",
        "                layers.append(nn.Dropout(0.5))\n",
        "                self.compress_layers.append(nn.Sequential(*layers))\n",
        "\n",
        "            self.fc1 = nn.Linear(2097, 1024)\n",
        "            self.relu1 = nn.ReLU()\n",
        "            self.dropout1 = nn.Dropout(0.5)\n",
        "            self.fc2 = nn.Linear(1024, 256)\n",
        "            self.relu2 = nn.ReLU()\n",
        "            self.dropout2 = nn.Dropout(0.5)\n",
        "            self.fc3 = nn.Linear(256, class_num)\n",
        "            self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "        def forward(self, input_l, input_b, input_r):\n",
        "            batch_size = input_l.shape[0]\n",
        "\n",
        "            # input_l 텐서를 첫 번째 차원을 기준으로 1 크기의 텐서로 분할\n",
        "            split_tensors = torch.split(input_l, 1, dim=1)\n",
        "            input = []\n",
        "\n",
        "            # 분할된 텐서들을 순회\n",
        "            for i, split_tensor in enumerate(split_tensors):\n",
        "                # 각 split_tensor를 배치 크기에 맞게 2차원으로 재구성\n",
        "                split_tensor = split_tensor.reshape(batch_size,-1)\n",
        "                # 재구성된 텐서를 압축(compress) layer를 거쳐 변환\n",
        "                input.append(self.compress_layers[i](split_tensor))\n",
        "\n",
        "            # input_b(bert 임베딩)와 input_r(Roberta 임베딩)을 input에 추가\n",
        "            input.append(input_b)\n",
        "            input.append(input_r)\n",
        "            input = torch.stack(input, dim=1)\n",
        "            # X * X^T\n",
        "            input_T = input.transpose(1, 2)\n",
        "            input_P = torch.matmul(input, input_T)\n",
        "            input_P = input_P.reshape(batch_size, -1)\n",
        "            # PN func\n",
        "            input_P = 2*F.sigmoid(self.SIGMA * input_P) - 1\n",
        "\n",
        "            a = torch.mean(input_l, dim=1)\n",
        "            input = torch.cat([input_P, a], dim=1)\n",
        "            # print(input.shape)\n",
        "\n",
        "            output = self.fc1(input)\n",
        "            output = self.relu1(output)\n",
        "            output = self.dropout1(output)\n",
        "            output = self.fc2(output)\n",
        "            output = self.relu2(output)\n",
        "            output = self.dropout2(output)\n",
        "            output = self.fc3(output)\n",
        "\n",
        "            # 소프트맥스 활성화 함수를 적용하여 클래스 확률을 출력\n",
        "            output = self.softmax(output)\n",
        "\n",
        "            return output\n",
        "\n",
        "\n",
        "\n",
        "    # Load the tokenizers and models for Llama2, BERT, and Roberta\n",
        "    llama2_tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\", use_auth_token=\"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\", trust_remote_code=True)\n",
        "    llama2_tokenizer.pad_token = llama2_tokenizer.eos_token  # 패딩 토큰 설정\n",
        "    llama2_config = AutoConfig.from_pretrained(\"meta-llama/Llama-3.2-1B\",use_auth_token=\"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\", output_hidden_states=True)\n",
        "    llama2_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\",use_auth_token=\"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\", config=llama2_config)\n",
        "\n",
        "    bert_tokenizer = BertTokenizer.from_pretrained('google-bert/bert-large-uncased')\n",
        "    bert_model = BertModel.from_pretrained('google-bert/bert-large-uncased')\n",
        "\n",
        "    roberta_tokenizer = RobertaTokenizer.from_pretrained('FacebookAI/roberta-large')\n",
        "    roberta_model = RobertaModel.from_pretrained('FacebookAI/roberta-large')\n",
        "\n",
        "    # Make sure all models are in evaluation mode and moved to the appropriate device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    llama2_model.eval().to(device)\n",
        "    bert_model.eval().to(device)\n",
        "    roberta_model.eval().to(device)\n",
        "\n",
        "    # Initialize the downstream model\n",
        "    class_num = 5  # For example, if you have 8 classes\n",
        "    SIGMA = 0.1  # SIGMA value for your downstream model\n",
        "    downstream_model = DownstreamModel(class_num, SIGMA).to(device)\n",
        "\n",
        "    model_load_path = \"/content/drive/MyDrive/LLMEmbed/model_weights_stackexchange_llama3_2.pth\"\n",
        "\n",
        "    # 가중치 로드\n",
        "    downstream_model.load_state_dict(torch.load(model_load_path, map_location=device))\n",
        "    downstream_model.eval()\n",
        "\n",
        "    def get_llama2_embedding(text, tokenizer, model, device):\n",
        "        inputs = tokenizer(text, return_tensors='pt', max_length=128, padding=\"max_length\", truncation=True).to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            # Average the last 5 layers\n",
        "            embedding = torch.stack([torch.mean(outputs.hidden_states[i], dim=1) for i in range(-1, -6, -1)], dim=1)\n",
        "        return embedding\n",
        "\n",
        "    def get_bert_embedding(text, tokenizer, model, device):\n",
        "        inputs = tokenizer(text, return_tensors='pt', max_length=128, padding=\"max_length\", truncation=True).to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            # Use pooler_output for BERT embeddings\n",
        "            embedding = outputs.pooler_output\n",
        "        return embedding\n",
        "\n",
        "    def get_roberta_embedding(text, tokenizer, model, device):\n",
        "        inputs = tokenizer(text, return_tensors='pt', max_length=128, padding=\"max_length\", truncation=True).to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            # Use the first token ([CLS] token) representation\n",
        "            embedding = outputs.last_hidden_state[:, 0, :]\n",
        "        return embedding\n",
        "\n",
        "    def infer(text, downstream_model, device):\n",
        "        # 각 모델로부터 임베딩을 추출\n",
        "        llama2_emb = get_llama2_embedding(text, llama2_tokenizer, llama2_model, device)\n",
        "        bert_emb = get_bert_embedding(text, bert_tokenizer, bert_model, device)\n",
        "        roberta_emb = get_roberta_embedding(text, roberta_tokenizer, roberta_model, device)\n",
        "\n",
        "        # Forward pass through the downstream model\n",
        "        with torch.no_grad():\n",
        "            prediction = downstream_model(llama2_emb, bert_emb, roberta_emb)\n",
        "\n",
        "        return prediction\n",
        "\n",
        "\n",
        "    # 레이블 맵\n",
        "    label_map = {\n",
        "        0: \"biology\",\n",
        "        1: \"cooking\",\n",
        "        2: \"diy\",\n",
        "        3: \"travel\",\n",
        "        4: \"stackoverflow\"\n",
        "    }\n",
        "\n",
        "    # Streamlit 앱의 레이아웃 설정\n",
        "    st.markdown('##### Single-Label Classification - LLMEmbed')\n",
        "    # st.title('LLMEmbed - llama3.2, roBERTa, BERT')\n",
        "\n",
        "\n",
        "    # 기본 텍스트 샘플\n",
        "    default_texts = [\n",
        "        \"What are the theoretical and actual (measured) minimum water potentials in plants?\",\n",
        "        \"What tastes like marigold? I'm looking to make Georgian Satsivi\",\n",
        "        \"How can I intentionally make my toilet make this noise? For Halloween I want to haunt my toilets and have them make the noise heard here:\",\n",
        "        \"What are the hours of operation of the Ankara Metro? What are the times of the first and last trains on the Ankara Metro?\",\n",
        "        \"How to use \\\"HTML form target self\\\" ? I am new to HTML and need to complete a simple task.\",\n",
        "    ]\n",
        "\n",
        "\n",
        "\n",
        "    # 입력 텍스트와 예측 결과 컨테이너\n",
        "    text_containers = []\n",
        "    result_label_containers = []\n",
        "    result_score_containers = []\n",
        "\n",
        "    # 컨테이너 생성 및 텍스트 입력\n",
        "    for i in range(5):\n",
        "        with st.container():\n",
        "            col1, col2, col3 = st.columns([3, 1, 1])\n",
        "            text = col1.text_area(\"Text\", value=default_texts[i], height=100, key=f\"text_{i}\")\n",
        "            text_containers.append(text)\n",
        "            # 예측 결과를 저장할 텍스트 박스 생성\n",
        "            result_label_container = col2.empty()\n",
        "            result_label_container.text_area(\"Predicted label\", \"\", height=100, key=f\"label_{i}\", disabled=True)\n",
        "            result_score_container = col3.empty()\n",
        "            result_score_container.text_area(\"Score\", \"\", height=100, key=f\"score_{i}\", disabled=True)\n",
        "            # 결과 컨테이너를 리스트에 추가\n",
        "            result_label_containers.append(result_label_container)\n",
        "            result_score_containers.append(result_score_container)\n",
        "\n",
        "    # 예측 버튼\n",
        "    if st.button('Classify All Texts'):\n",
        "        # 모든 텍스트에 대해 예측 실행\n",
        "        for i, text in enumerate(text_containers):\n",
        "            prediction = infer(text, downstream_model, device)\n",
        "            predicted_label = torch.argmax(prediction, dim=1).item()\n",
        "            predicted_score = torch.max(prediction).item()\n",
        "            label_name = label_map[predicted_label]\n",
        "\n",
        "            # 예측 결과를 각 텍스트 박스에 작성 (수정된 코드)\n",
        "            result_label_containers[i].text_area(\"Category\", value=label_name, height=100, key=f\"updated_label_{i}\", disabled=True)\n",
        "            result_score_containers[i].text_area(\"Score\", value=f\"{predicted_score:.4f}\", height=100, key=f\"updated_score_{i}\", disabled=True)\n",
        "\n",
        "elif classification_type == 'Multi-Label Classification':\n",
        "\n",
        "\n",
        "    @dataclass\n",
        "    class MambaConfig:\n",
        "        d_model: int = 768\n",
        "        d_intermediate: int = 0\n",
        "        n_layer: int = 24\n",
        "        vocab_size: int = 50277\n",
        "        ssm_cfg: dict = None\n",
        "        attn_layer_idx: list = None\n",
        "        attn_cfg: dict = None\n",
        "        rms_norm: bool = True\n",
        "        residual_in_fp32: bool = True\n",
        "        fused_add_norm: bool = True\n",
        "        pad_vocab_size_multiple: int = 16\n",
        "        tie_embeddings: bool = True\n",
        "\n",
        "        def __post_init__(self):\n",
        "            # 기본값으로 설정된 None 타입을 빈 딕셔너리와 빈 리스트로 초기화\n",
        "            if self.ssm_cfg is None:\n",
        "                self.ssm_cfg = {\"layer\": \"Mamba2\"}\n",
        "            if self.attn_layer_idx is None:\n",
        "                self.attn_layer_idx = []\n",
        "            if self.attn_cfg is None:\n",
        "                self.attn_cfg = {}\n",
        "\n",
        "        def to_json_string(self):\n",
        "            return json.dumps(asdict(self))\n",
        "\n",
        "        def to_dict(self):\n",
        "            return asdict(self)\n",
        "\n",
        "\n",
        "    classes = ['Algorithms', 'Backend', 'Data Science', 'Databases', 'Dev Tools', 'Frontend', 'Mobile', 'Systems', 'iOS/macOS']\n",
        "    class2id = {'Algorithms' :0, 'Backend' : 1, 'Data Science' : 2, 'Databases' : 3, 'Dev Tools' : 4, 'Frontend' : 5, 'Mobile' :6, 'Systems' : 7, 'iOS/macOS' : 8}\n",
        "    id2class = {0 : 'Algorithms', 1: 'Backend', 2 : 'Data Science', 3 : 'Databases', 4 : 'Dev Tools', 5 : 'Frontend', 6 : 'Mobile', 7 : 'Systems', 8 :'iOS/macOS'}\n",
        "\n",
        "\n",
        "    class MambaClassificationHead(nn.Module):\n",
        "        def __init__(self, d_model, num_classes, **kwargs):\n",
        "            super(MambaClassificationHead, self).__init__()\n",
        "            self.classification_head = nn.Linear(d_model, num_classes, **kwargs)\n",
        "\n",
        "        def forward(self, hidden_states):\n",
        "            return self.classification_head(hidden_states)\n",
        "\n",
        "    class MambaTextClassification(MambaLMHeadModel):\n",
        "        def __init__(\n",
        "            self,\n",
        "            config: MambaConfig,\n",
        "            initializer_cfg=None,\n",
        "            device=None,\n",
        "            dtype=None,\n",
        "        ) -> None:\n",
        "            super().__init__(config, initializer_cfg, device, dtype)\n",
        "\n",
        "            self.classification_head = MambaClassificationHead(d_model=config.d_model, num_classes=len(classes))\n",
        "            del self.lm_head\n",
        "            self.multi_label = True\n",
        "            self.id2label = id2class\n",
        "            self.class2id = class2id\n",
        "\n",
        "        @classmethod\n",
        "        def addMambaClassificationHead(cls, num_classes, id2label, class2id, multi_label):\n",
        "            cls.classification_head = MambaClassificationHead\n",
        "            del self.lm_head\n",
        "\n",
        "        def forward(self, input_ids, attention_mask=None, labels=None):\n",
        "            hidden_states = self.backbone(input_ids)\n",
        "            mean_hidden_states = hidden_states.mean(dim=1)\n",
        "\n",
        "            logits = self.classification_head(mean_hidden_states)\n",
        "            if labels is None:\n",
        "              ClassificationOutput = namedtuple(\"ClassificationOutput\", [\"logits\"])\n",
        "              return ClassificationOutput(logits=logits)\n",
        "            else:\n",
        "              ClassificationOutput = namedtuple(\"ClassificationOutput\", [\"loss\", \"logits\"])\n",
        "              if self.multi_label:\n",
        "                loss_fct = nn.BCEWithLogitsLoss()\n",
        "                loss = loss_fct(logits, labels)\n",
        "              else:\n",
        "                loss_fct = nn.CrossEntropyLoss()\n",
        "                loss = loss_fct(logits, labels)\n",
        "              return ClassificationOutput(loss=loss, logits=logits)\n",
        "\n",
        "        def predict(self, text, tokenizer):\n",
        "            input_ids = torch.tensor(tokenizer(text)['input_ids'], device='cuda')[None]\n",
        "            with torch.no_grad():\n",
        "              logits = self.forward(input_ids).logits[0]\n",
        "\n",
        "            if self.multi_label:\n",
        "              probabilities = torch.sigmoid(logits).cpu().numpy()\n",
        "              predictions = (probabilities > 0.5).astype(int)\n",
        "              return [self.id2label[i] for i, value in enumerate(predictions) if value == 1]\n",
        "            else:\n",
        "              label = np.argmax(logits.cpu().numpy())\n",
        "              return self.id2label[label]\n",
        "\n",
        "        @classmethod\n",
        "        def from_pretrained(cls, pretrained_model_name, device=None, dtype=None, **kwargs):\n",
        "            config_data = load_config_hf(pretrained_model_name)\n",
        "            config = MambaConfig(**config_data)\n",
        "\n",
        "            model = cls(config, device=device, dtype=dtype, **kwargs)\n",
        "\n",
        "            model_state_dict = load_state_dict_hf(pretrained_model_name, device=device, dtype=dtype)\n",
        "            model.load_state_dict(model_state_dict, strict=False)\n",
        "\n",
        "            print(\"Newly initialized embedding:\", set(model.state_dict().keys()) - set(model_state_dict.keys()))\n",
        "            return model\n",
        "\n",
        "    # Streamlit 앱의 제목 설정\n",
        "    st.markdown('##### Multi-Label Classification - mamba2')\n",
        "    # st.title('Mamba2')\n",
        "\n",
        "    # 레이아웃을 2개의 컬럼으로 분할\n",
        "    col1, col2 = st.columns(2)\n",
        "\n",
        "    # 사용자로부터 입력 받을 텍스트의 예시\n",
        "    example_text = \"\"\"OAuth 2.0 Not Working Properly with Kakao Login in React App\n",
        "    I'm a computer science student\n",
        "\n",
        "    I'm sorry if there are any typos in my english\n",
        "\n",
        "    If you need more information my code ask to me\n",
        "\n",
        "    I’m implementing Kakao and Naver login with OAuth 2.0 in my React app. The login process appears to succeed, and the URL changes to indicate successful login (with the code parameter in the URL), but no user information is being logged to the console, and nothing is stored in local storage. Additionally, I’m not seeing any console output from console.log statements in my callback component.\n",
        "    \"\"\"\n",
        "\n",
        "    with col1:  # 첫 번째 컬럼에 입력 필드 생성\n",
        "        text = st.text_area(\"Text\", value=example_text, height=300)\n",
        "\n",
        "    # 모델 및 토크나이저 로드\n",
        "    # @st.cache(allow_output_mutation=True)  # Streamlit 캐시를 사용하여 모델을 한 번만 로드\n",
        "    def load_model_and_tokenizer():\n",
        "        model = MambaTextClassification.from_pretrained(\"ebinna/multi_cls_mamba2-130m\")\n",
        "        model.to(\"cuda\")\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\"ebinna/multi_cls_mamba2-130m\")\n",
        "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "        return model, tokenizer\n",
        "\n",
        "    model, tokenizer = load_model_and_tokenizer()\n",
        "\n",
        "    # 초기 결과값을 빈 문자열로 설정\n",
        "    result_text = \"\"\n",
        "\n",
        "    # 분류 실행 버튼\n",
        "    if st.button('Classify Text'):\n",
        "        # 텍스트 예측\n",
        "        prediction = model.predict(text, tokenizer)\n",
        "        result_text = str(prediction)\n",
        "\n",
        "    with col2:  # 두 번째 컬럼에 결과 레이블과 텍스트 영역을 항상 표시\n",
        "        st.text_area(\"Tags\", value=result_text, height=300, disabled=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWkW3btqI9PD",
        "outputId": "fd016961-350e-4bcd-c35f-211a963dd8d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install localtunnel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUoxBgxaOggI",
        "outputId": "03bef9d7-66e0-40e3-81f0-577ddb53697f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25h\n",
            "added 22 packages, and audited 23 packages in 1s\n",
            "\n",
            "3 packages are looking for funding\n",
            "  run `npm fund` for details\n",
            "\n",
            "2 \u001b[33m\u001b[1mmoderate\u001b[22m\u001b[39m severity vulnerabilities\n",
            "\n",
            "To address all issues, run:\n",
            "  npm audit fix\n",
            "\n",
            "Run `npm audit` for details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run /content/app.py &>/content/drive/MyDrive/logs.txt & npx localtunnel --port 8501 & curl ipv4.icanhazip.com"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9FyvWqLFOkYj",
        "outputId": "d561d607-118c-4e84-881d-2d7ea1c0feb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "35.227.33.188\n",
            "your url is: https://pink-actors-go.loca.lt\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "A100",
      "mount_file_id": "1gEmMWIh10KFTtFUNkiBBWm6CIsXaIqwR",
      "authorship_tag": "ABX9TyOLWBh711JFiii9bCYWhdY3",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}