{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMjeB0G81oX464v4jcATsWy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jeremy-su1/ai-algorithm/blob/main/final/multi_cls_sobert_base_predict.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HjmfLNaI_Tsw"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install datasets>=2.18.0 transformers>=4.38.2 sentence-transformers>=2.5.1 setfit>=1.0.3 accelerate>=0.27.2 seqeval>=1.2.2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TnHOxVeN_1OV",
        "outputId": "528f3b3c-d825-4f29-9b9c-2c9c0f2cd569"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = '/content/drive/My Drive/multi_cls_sobert_base/multi_cls_sobert_base'  # 모델과 토크나이저가 저장된 경로"
      ],
      "metadata": {
        "id": "SNyWbkaF349Y"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes = ['Algorithms', 'Backend', 'Data Science', 'Databases', 'Dev Tools', 'Frontend', 'Mobile', 'Systems', 'iOS/macOS']\n",
        "class2id = {'Algorithms' :0, 'Backend' : 1, 'Data Science' : 2, 'Databases' : 3, 'Dev Tools' : 4, 'Frontend' : 5, 'Mobile' :6, 'Systems' : 7, 'iOS/macOS' : 8}\n",
        "id2class = {0 : 'Algorithms', 1: 'Backend', 2 : 'Data Science', 3 : 'Databases', 4 : 'Dev Tools', 5 : 'Frontend', 6 : 'Mobile', 7 : 'Systems', 8 :'iOS/macOS'}"
      ],
      "metadata": {
        "id": "DVvqrjUrDnJW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelForSequenceClassification"
      ],
      "metadata": {
        "id": "WqlErohjE2hV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_path)"
      ],
      "metadata": {
        "id": "RZgTFY1RU0Ke"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-MAcrxF7Apx",
        "outputId": "32b37c5d-65d8-434d-da4b-8dab26aedf0a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MegatronBertForSequenceClassification(\n",
              "  (bert): MegatronBertModel(\n",
              "    (embeddings): MegatronBertEmbeddings(\n",
              "      (word_embeddings): Embedding(50048, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(2048, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): MegatronBertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x MegatronBertLayer(\n",
              "          (attention): MegatronBertAttention(\n",
              "            (ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (self): MegatronBertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): MegatronBertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (intermediate): MegatronBertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): MegatronBertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    )\n",
              "    (pooler): MegatronBertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=9, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 예측 함수 정의\n",
        "def predict(texts, m, tk):\n",
        "    # 텍스트를 토큰화하고 텐서로 변환\n",
        "    inputs = tk(texts, padding='max_length', truncation=True, max_length=512, return_tensors='pt').to(m.device)\n",
        "\n",
        "    # 모델을 사용해 예측 수행\n",
        "    with torch.no_grad():\n",
        "        outputs = m(**inputs)\n",
        "        logits = outputs.logits[0]\n",
        "\n",
        "    probabilities = torch.sigmoid(logits).cpu().numpy()\n",
        "    predictions = (probabilities > 0.5).astype(int)\n",
        "    result = [id2class[i] for i, value in enumerate(predictions) if value == 1]\n",
        "    return result"
      ],
      "metadata": {
        "id": "Iz8s4cUuVBW6"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 샘플 텍스트 입력 및 예측 수행\n",
        "text =  \"\"\"does laragon have backup folder for MySQL like Xampp or am i not safe?\n",
        "\n",
        "i switched from XAMPP to laragon because the mysql fails 2 times a week no matter what i do and i keep getting inconsistent errors that suggest systematic corruption of the mysql and despite trying trying to reinstall XAMPP before it seems like no matter what i do the problem arises after a while no matter how long MySQL stays stable it fails after i can drop my error logs if anyone is interested or have any knowledge of this peculiar behavior anyways i don't if it makes a difference but i switched to laragon and it has cleaner interface and felt more simpler however one thing i noticed in the MySQL folder is that there is no backup folder, is this normal?\n",
        "\n",
        "I tried going into the Laragoon folder and found no backup folder to MySQL\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# 예측 수행\n",
        "print(predict(text, model, tokenizer))"
      ],
      "metadata": {
        "id": "a2s5Z_fvVWHH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3b21ec7-d3b7-41bf-f345-4f989bfe7da1"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Backend', 'Databases']\n"
          ]
        }
      ]
    }
  ]
}