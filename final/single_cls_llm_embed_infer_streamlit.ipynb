{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jeremy-su1/ai-algorithm/blob/main/final/single_cls_llm_embed_infer_streamlit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qdqdxUK_0-h",
        "outputId": "30216564-42ed-4b15-d40b-8ea09dc380db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.6)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec (from torch)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
            "Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.1.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torch transformers tqdm datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q streamlit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzlUHzTcMkwo",
        "outputId": "787d1c5c-c54a-4f60-fd20-525a52edbccc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m70.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m116.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import streamlit as st\n",
        "import torch\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BertTokenizer, BertModel, RobertaTokenizer, RobertaModel\n",
        "from transformers import AutoConfig\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class DownstreamModel(nn.Module):\n",
        "    def __init__(self, class_num, SIGMA):\n",
        "        super(DownstreamModel, self).__init__()\n",
        "        self.SIGMA = SIGMA\n",
        "        self.compress_layers = nn.ModuleList()\n",
        "        for _ in range(5):\n",
        "            layers = []\n",
        "            layers.append(nn.Linear(2048, 1024))\n",
        "            layers.append(nn.ReLU())\n",
        "            layers.append(nn.Dropout(0.5))\n",
        "            self.compress_layers.append(nn.Sequential(*layers))\n",
        "\n",
        "        self.fc1 = nn.Linear(2097, 1024)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.dropout1 = nn.Dropout(0.5)\n",
        "        self.fc2 = nn.Linear(1024, 256)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "        self.fc3 = nn.Linear(256, class_num)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, input_l, input_b, input_r):\n",
        "        batch_size = input_l.shape[0]\n",
        "\n",
        "        # input_l 텐서를 첫 번째 차원을 기준으로 1 크기의 텐서로 분할\n",
        "        split_tensors = torch.split(input_l, 1, dim=1)\n",
        "        input = []\n",
        "\n",
        "        # 분할된 텐서들을 순회\n",
        "        for i, split_tensor in enumerate(split_tensors):\n",
        "            # 각 split_tensor를 배치 크기에 맞게 2차원으로 재구성\n",
        "            split_tensor = split_tensor.reshape(batch_size,-1)\n",
        "            # 재구성된 텐서를 압축(compress) layer를 거쳐 변환\n",
        "            input.append(self.compress_layers[i](split_tensor))\n",
        "\n",
        "         # input_b(bert 임베딩)와 input_r(Roberta 임베딩)을 input에 추가\n",
        "        input.append(input_b)\n",
        "        input.append(input_r)\n",
        "        input = torch.stack(input, dim=1)\n",
        "        # X * X^T\n",
        "        input_T = input.transpose(1, 2)\n",
        "        input_P = torch.matmul(input, input_T)\n",
        "        input_P = input_P.reshape(batch_size, -1)\n",
        "        # PN func\n",
        "        input_P = 2*F.sigmoid(self.SIGMA * input_P) - 1\n",
        "\n",
        "        a = torch.mean(input_l, dim=1)\n",
        "        input = torch.cat([input_P, a], dim=1)\n",
        "        # print(input.shape)\n",
        "\n",
        "        output = self.fc1(input)\n",
        "        output = self.relu1(output)\n",
        "        output = self.dropout1(output)\n",
        "        output = self.fc2(output)\n",
        "        output = self.relu2(output)\n",
        "        output = self.dropout2(output)\n",
        "        output = self.fc3(output)\n",
        "\n",
        "        # 소프트맥스 활성화 함수를 적용하여 클래스 확률을 출력\n",
        "        output = self.softmax(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "\n",
        "# Load the tokenizers and models for Llama2, BERT, and Roberta\n",
        "llama2_tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\", use_auth_token=\"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\", trust_remote_code=True)\n",
        "llama2_tokenizer.pad_token = llama2_tokenizer.eos_token  # 패딩 토큰 설정\n",
        "llama2_config = AutoConfig.from_pretrained(\"meta-llama/Llama-3.2-1B\",use_auth_token=\"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\", output_hidden_states=True)\n",
        "llama2_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\",use_auth_token=\"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\", config=llama2_config)\n",
        "\n",
        "bert_tokenizer = BertTokenizer.from_pretrained('google-bert/bert-large-uncased')\n",
        "bert_model = BertModel.from_pretrained('google-bert/bert-large-uncased')\n",
        "\n",
        "roberta_tokenizer = RobertaTokenizer.from_pretrained('FacebookAI/roberta-large')\n",
        "roberta_model = RobertaModel.from_pretrained('FacebookAI/roberta-large')\n",
        "\n",
        "# Make sure all models are in evaluation mode and moved to the appropriate device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "llama2_model.eval().to(device)\n",
        "bert_model.eval().to(device)\n",
        "roberta_model.eval().to(device)\n",
        "\n",
        "# Initialize the downstream model\n",
        "class_num = 5  # For example, if you have 8 classes\n",
        "SIGMA = 0.1  # SIGMA value for your downstream model\n",
        "downstream_model = DownstreamModel(class_num, SIGMA).to(device)\n",
        "\n",
        "model_load_path = \"/content/drive/MyDrive/LLMEmbed/model_weights_stackexchange_llama3_2.pth\"\n",
        "\n",
        "# 가중치 로드\n",
        "downstream_model.load_state_dict(torch.load(model_load_path, map_location=device))\n",
        "downstream_model.eval()\n",
        "\n",
        "def get_llama2_embedding(text, tokenizer, model, device):\n",
        "    inputs = tokenizer(text, return_tensors='pt', max_length=128, padding=\"max_length\", truncation=True).to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        # Average the last 5 layers\n",
        "        embedding = torch.stack([torch.mean(outputs.hidden_states[i], dim=1) for i in range(-1, -6, -1)], dim=1)\n",
        "    return embedding\n",
        "\n",
        "def get_bert_embedding(text, tokenizer, model, device):\n",
        "    inputs = tokenizer(text, return_tensors='pt', max_length=128, padding=\"max_length\", truncation=True).to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        # Use pooler_output for BERT embeddings\n",
        "        embedding = outputs.pooler_output\n",
        "    return embedding\n",
        "\n",
        "def get_roberta_embedding(text, tokenizer, model, device):\n",
        "    inputs = tokenizer(text, return_tensors='pt', max_length=128, padding=\"max_length\", truncation=True).to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        # Use the first token ([CLS] token) representation\n",
        "        embedding = outputs.last_hidden_state[:, 0, :]\n",
        "    return embedding\n",
        "\n",
        "def infer(text, downstream_model, device):\n",
        "    # 각 모델로부터 임베딩을 추출\n",
        "    llama2_emb = get_llama2_embedding(text, llama2_tokenizer, llama2_model, device)\n",
        "    bert_emb = get_bert_embedding(text, bert_tokenizer, bert_model, device)\n",
        "    roberta_emb = get_roberta_embedding(text, roberta_tokenizer, roberta_model, device)\n",
        "\n",
        "    # Forward pass through the downstream model\n",
        "    with torch.no_grad():\n",
        "        prediction = downstream_model(llama2_emb, bert_emb, roberta_emb)\n",
        "\n",
        "    return prediction\n",
        "\n",
        "\n",
        "# 레이블 맵\n",
        "label_map = {\n",
        "    0: \"biology\",\n",
        "    1: \"cooking\",\n",
        "    2: \"diy\",\n",
        "    3: \"travel\",\n",
        "    4: \"stackoverflow\"\n",
        "}\n",
        "\n",
        "\n",
        "# Streamlit 앱의 레이아웃 설정\n",
        "st.title('Batch Text Classification App')\n",
        "\n",
        "# 기본 텍스트 샘플\n",
        "default_texts = [\n",
        "    \"What are the theoretical and actual (measured) minimum water potentials in plants?\",\n",
        "    \"What tastes like marigold? I'm looking to make Georgian Satsivi\",\n",
        "    \"How can I intentionally make my toilet make this noise? For Halloween I want to haunt my toilets and have them make the noise heard here:\",\n",
        "    \"What are the hours of operation of the Ankara Metro? What are the times of the first and last trains on the Ankara Metro?\",\n",
        "    \"How to use \\\"HTML form target self\\\" ? I am new to HTML and need to complete a simple task.\",\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "# 입력 텍스트와 예측 결과 컨테이너\n",
        "text_containers = []\n",
        "result_label_containers = []\n",
        "result_score_containers = []\n",
        "\n",
        "# 컨테이너 생성 및 텍스트 입력\n",
        "for i in range(5):\n",
        "    with st.container():\n",
        "        col1, col2, col3 = st.columns([3, 1, 1])\n",
        "        text = col1.text_area(\"Text\", value=default_texts[i], height=100, key=f\"text_{i}\")\n",
        "        text_containers.append(text)\n",
        "        # 예측 결과를 저장할 텍스트 박스 생성\n",
        "        result_label_container = col2.empty()\n",
        "        result_label_container.text_area(\"Predicted label\", \"\", height=100, key=f\"label_{i}\", disabled=True)\n",
        "        result_score_container = col3.empty()\n",
        "        result_score_container.text_area(\"Score\", \"\", height=100, key=f\"score_{i}\", disabled=True)\n",
        "        # 결과 컨테이너를 리스트에 추가\n",
        "        result_label_containers.append(result_label_container)\n",
        "        result_score_containers.append(result_score_container)\n",
        "\n",
        "# 예측 버튼\n",
        "if st.button('Classify All Texts'):\n",
        "    # 모든 텍스트에 대해 예측 실행\n",
        "    for i, text in enumerate(text_containers):\n",
        "        prediction = infer(text, downstream_model, device)\n",
        "        predicted_label = torch.argmax(prediction, dim=1).item()\n",
        "        predicted_score = torch.max(prediction).item()\n",
        "        label_name = label_map[predicted_label]\n",
        "\n",
        "        # 예측 결과를 각 텍스트 박스에 작성 (수정된 코드)\n",
        "        # 레이블 문자열을 유지하면서 값을 업데이트합니다.\n",
        "        result_label_containers[i].text_area(\"Predicted label\", value=label_name, height=100, key=f\"updated_label_{i}\", disabled=True)\n",
        "        result_score_containers[i].text_area(\"Score\", value=f\"{predicted_score:.4f}\", height=100, key=f\"updated_score_{i}\", disabled=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWkW3btqI9PD",
        "outputId": "86a9406a-c422-4b46-f11c-195a382e64f4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install localtunnel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUoxBgxaOggI",
        "outputId": "3d451649-7b39-4949-f9e2-65f82bb33bfe"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25h\n",
            "added 22 packages, and audited 23 packages in 1s\n",
            "\n",
            "3 packages are looking for funding\n",
            "  run `npm fund` for details\n",
            "\n",
            "2 \u001b[33m\u001b[1mmoderate\u001b[22m\u001b[39m severity vulnerabilities\n",
            "\n",
            "To address all issues, run:\n",
            "  npm audit fix\n",
            "\n",
            "Run `npm audit` for details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run /content/app.py &>/content/drive/MyDrive/logs.txt & npx localtunnel --port 8501 & curl ipv4.icanhazip.com"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9FyvWqLFOkYj",
        "outputId": "122e5a6b-d31e-45d3-dade-50ebb98b19b2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "35.232.48.246\n",
            "your url is: https://breezy-parrots-dance.loca.lt\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "A100",
      "mount_file_id": "1qYvVxLJ0Rc5eXbjdxPEDBe-3bl2l7rUf",
      "authorship_tag": "ABX9TyMihDDXAWqmDGxgcWXZAZ4k",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}