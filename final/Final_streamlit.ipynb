{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyNjaFuzZ+EPR+daAaxQOaYS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b4314cb7cde048e78d2b4c4fef3da452": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_ee28861d7b9842949c8776247c5b403f"
          }
        },
        "8d0cd4dea982483e97534873043aa85d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c16d64546f4b4b0f99d4af5f4afb74c9",
            "placeholder": "​",
            "style": "IPY_MODEL_76741bcd6dc2477dbbf83d1a47b9cc00",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "76a22e7f9c16495f9a0599bb268478ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_8f411b762c29426f84b4616b57b029b5",
            "placeholder": "​",
            "style": "IPY_MODEL_86aa60a5db094e5fa942c88a7855eae1",
            "value": ""
          }
        },
        "2e2f500cc4ec4498b842d7ce7c1080c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_7f9966fda08c43d1be8c1f4591c8bf62",
            "style": "IPY_MODEL_abcc85209c0e440cbb0609ba548f9769",
            "value": true
          }
        },
        "c4d11f4bf7b049e1b1ee42bba12befea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_9207705c14bb4ea8b04d349cafb72947",
            "style": "IPY_MODEL_630b9f43c491420fbcd9522dd6010bd8",
            "tooltip": ""
          }
        },
        "2372eca36e23455ca87a51514a585ea9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7189f54bd7534d398f5785e26a401942",
            "placeholder": "​",
            "style": "IPY_MODEL_60ab463e30c8488eb8d07b33a21525dc",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "ee28861d7b9842949c8776247c5b403f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "c16d64546f4b4b0f99d4af5f4afb74c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "76741bcd6dc2477dbbf83d1a47b9cc00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8f411b762c29426f84b4616b57b029b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86aa60a5db094e5fa942c88a7855eae1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7f9966fda08c43d1be8c1f4591c8bf62": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "abcc85209c0e440cbb0609ba548f9769": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9207705c14bb4ea8b04d349cafb72947": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "630b9f43c491420fbcd9522dd6010bd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "7189f54bd7534d398f5785e26a401942": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "60ab463e30c8488eb8d07b33a21525dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "70856cbc7bd249669e0189735977322c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1c8425175bbe40f98a18390dc58564af",
            "placeholder": "​",
            "style": "IPY_MODEL_07d980bdbb8241539b91ea550dea2005",
            "value": "Connecting..."
          }
        },
        "1c8425175bbe40f98a18390dc58564af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "07d980bdbb8241539b91ea550dea2005": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jeremy-su1/ai-algorithm/blob/main/final/Final_streamlit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74oOOYOSefTC",
        "outputId": "e73a4dbf-2677-4759-b944-bcd8b13fcf12"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch transformers tqdm datasets\n",
        "!pip install torch==2.1.1 torchvision==0.16.1 torchaudio==2.1.1 --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install causal-conv1d>=1.1.0\n",
        "!pip install mamba-ssm\n",
        "!pip install datasets evaluate accelerate\n",
        "!pip install huggingface_hub\n",
        "!export LC_ALL=\"en_US.UTF-8\"\n",
        "!export LD_LIBRARY_PATH=\"/usr/lib64-nvidia\"\n",
        "!export LIBRARY_PATH=\"/usr/local/cuda/lib64/stubs\"\n",
        "!ldconfig /usr/lib64-nvidia"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3atA_QwImAaQ",
        "outputId": "b270e972-dfd8-4ed1-ffc2-bc263bc634e0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.6)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec (from torch)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
            "Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.1.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Collecting torch==2.1.1\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torch-2.1.1%2Bcu118-cp310-cp310-linux_x86_64.whl (2325.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 GB\u001b[0m \u001b[31m795.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.16.1\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.16.1%2Bcu118-cp310-cp310-linux_x86_64.whl (6.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m98.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchaudio==2.1.1\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.1.1%2Bcu118-cp310-cp310-linux_x86_64.whl (3.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m74.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.1.1) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.1.1) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.1.1) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.1.1) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.1) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.1.1) (2024.9.0)\n",
            "Collecting triton==2.1.0 (from torch==2.1.1)\n",
            "  Downloading https://download.pytorch.org/whl/triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.16.1) (1.26.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision==0.16.1) (2.32.3)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.16.1) (11.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.1.1) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.16.1) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.16.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.16.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.16.1) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.1.1) (1.3.0)\n",
            "Installing collected packages: triton, torch, torchvision, torchaudio\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.5.0+cu121\n",
            "    Uninstalling torch-2.5.0+cu121:\n",
            "      Successfully uninstalled torch-2.5.0+cu121\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.20.0+cu121\n",
            "    Uninstalling torchvision-0.20.0+cu121:\n",
            "      Successfully uninstalled torchvision-0.20.0+cu121\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.5.0+cu121\n",
            "    Uninstalling torchaudio-2.5.0+cu121:\n",
            "      Successfully uninstalled torchaudio-2.5.0+cu121\n",
            "Successfully installed torch-2.1.1+cu118 torchaudio-2.1.1+cu118 torchvision-0.16.1+cu118 triton-2.1.0\n",
            "Collecting mamba-ssm\n",
            "  Downloading mamba_ssm-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from mamba-ssm) (2.1.1+cu118)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from mamba-ssm) (24.2)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from mamba-ssm) (1.11.1.1)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from mamba-ssm) (0.8.0)\n",
            "Requirement already satisfied: triton in /usr/local/lib/python3.10/dist-packages (from mamba-ssm) (2.1.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from mamba-ssm) (4.46.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (2024.9.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (1.26.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (0.20.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (4.66.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->mamba-ssm) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->mamba-ssm) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->mamba-ssm) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->mamba-ssm) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->mamba-ssm) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->mamba-ssm) (1.3.0)\n",
            "Building wheels for collected packages: mamba-ssm\n",
            "  Building wheel for mamba-ssm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mamba-ssm: filename=mamba_ssm-2.2.2-cp310-cp310-linux_x86_64.whl size=343412612 sha256=150be68a91b3444b35ee2cda6a4790aaf83711009a8bf8e91afc7a316b091df6\n",
            "  Stored in directory: /root/.cache/pip/wheels/57/7c/90/9f963468ecc3791e36e388f9e7b4a4e1e3f90fbb340055aa4d\n",
            "Successfully built mamba-ssm\n",
            "Installing collected packages: mamba-ssm\n",
            "Successfully installed mamba-ssm-2.2.2\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.1.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.1+cu118)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: evaluate\n",
            "Successfully installed evaluate-0.4.3\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.26.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2024.9.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2024.8.30)\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!export LC_ALL=\"en_US.UTF-8\"\n",
        "!export LD_LIBRARY_PATH=\"/usr/lib64-nvidia\"\n",
        "!export LIBRARY_PATH=\"/usr/local/cuda/lib64/stubs\"\n",
        "!ldconfig /usr/lib64-nvidia"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GM5SvEiW7yCG",
        "outputId": "68eddfe1-8b3d-4f71-9d50-f1e2c8c6cdea"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1VgPLaUHeKF5",
        "outputId": "601dd2f6-4adb-46c4-bdae-2f499d3bc9d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.10/dist-packages (7.2.1)\n",
            "Collecting streamlit\n",
            "  Downloading streamlit-1.40.1-py2.py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\n",
            "Requirement already satisfied: numpy<3,>=1.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.26.4)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (24.2)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (11.0.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.25.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (17.0.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.9.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.0.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.12.2)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.1.43)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.3)\n",
            "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.12.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.11)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2024.8.30)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.18.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.21.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.16.0)\n",
            "Downloading streamlit-1.40.1-py2.py3-none-any.whl (8.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m68.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m72.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: watchdog, pydeck, streamlit\n",
            "Successfully installed pydeck-0.9.1 streamlit-1.40.1 watchdog-6.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pyngrok streamlit"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "b4314cb7cde048e78d2b4c4fef3da452",
            "8d0cd4dea982483e97534873043aa85d",
            "76a22e7f9c16495f9a0599bb268478ac",
            "2e2f500cc4ec4498b842d7ce7c1080c9",
            "c4d11f4bf7b049e1b1ee42bba12befea",
            "2372eca36e23455ca87a51514a585ea9",
            "ee28861d7b9842949c8776247c5b403f",
            "c16d64546f4b4b0f99d4af5f4afb74c9",
            "76741bcd6dc2477dbbf83d1a47b9cc00",
            "8f411b762c29426f84b4616b57b029b5",
            "86aa60a5db094e5fa942c88a7855eae1",
            "7f9966fda08c43d1be8c1f4591c8bf62",
            "abcc85209c0e440cbb0609ba548f9769",
            "9207705c14bb4ea8b04d349cafb72947",
            "630b9f43c491420fbcd9522dd6010bd8",
            "7189f54bd7534d398f5785e26a401942",
            "60ab463e30c8488eb8d07b33a21525dc",
            "70856cbc7bd249669e0189735977322c",
            "1c8425175bbe40f98a18390dc58564af",
            "07d980bdbb8241539b91ea550dea2005"
          ]
        },
        "id": "aPABtWpOU0NW",
        "outputId": "f77e734e-7f5d-4522-a973-a065eb1d766b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b4314cb7cde048e78d2b4c4fef3da452"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForCausalLM, BertTokenizer, BertModel, RobertaTokenizer, RobertaModel\n",
        "from transformers import AutoConfig\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "\n",
        "import random\n",
        "import json\n",
        "from collections import namedtuple\n",
        "from dataclasses import dataclass, field, asdict\n",
        "from mamba_ssm.models.mixer_seq_simple import MambaLMHeadModel\n",
        "from mamba_ssm.utils.hf import load_config_hf, load_state_dict_hf\n",
        "\n",
        "import evaluate\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from transformers import Trainer\n",
        "from transformers import AutoTokenizer, TrainingArguments\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()\n",
        "token = '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'\n",
        "#token =\n",
        "# 장치 설정\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# LLMEmbed 모델 정의\n",
        "class DownstreamModel(nn.Module):\n",
        "    def __init__(self, class_num, SIGMA):\n",
        "        super(DownstreamModel, self).__init__()\n",
        "        self.SIGMA = SIGMA\n",
        "        self.compress_layers = nn.ModuleList()\n",
        "        for _ in range(5):\n",
        "            layers = []\n",
        "            layers.append(nn.Linear(2048, 1024))\n",
        "            layers.append(nn.ReLU())\n",
        "            layers.append(nn.Dropout(0.5))\n",
        "            self.compress_layers.append(nn.Sequential(*layers))\n",
        "\n",
        "        self.fc1 = nn.Linear(2097, 1024)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.dropout1 = nn.Dropout(0.5)\n",
        "        self.fc2 = nn.Linear(1024, 256)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "        self.fc3 = nn.Linear(256, class_num)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, input_l, input_b, input_r):\n",
        "        batch_size = input_l.shape[0]\n",
        "\n",
        "        split_tensors = torch.split(input_l, 1, dim=1)\n",
        "        input = []\n",
        "        for i, split_tensor in enumerate(split_tensors):\n",
        "            split_tensor = split_tensor.reshape(batch_size, -1)\n",
        "            input.append(self.compress_layers[i](split_tensor))\n",
        "\n",
        "        input.append(input_b)\n",
        "        input.append(input_r)\n",
        "        input = torch.stack(input, dim=1)\n",
        "        input_T = input.transpose(1, 2)\n",
        "        input_P = torch.matmul(input, input_T)\n",
        "        input_P = input_P.reshape(batch_size, -1)\n",
        "        input_P = 2 * F.sigmoid(self.SIGMA * input_P) - 1\n",
        "\n",
        "        a = torch.mean(input_l, dim=1)\n",
        "        input = torch.cat([input_P, a], dim=1)\n",
        "\n",
        "        output = self.fc1(input)\n",
        "        output = self.relu1(output)\n",
        "        output = self.dropout1(output)\n",
        "        output = self.fc2(output)\n",
        "        output = self.relu2(output)\n",
        "        output = self.dropout2(output)\n",
        "        output = self.fc3(output)\n",
        "        output = self.softmax(output)\n",
        "        return output\n",
        "\n",
        "## mamba ##\n",
        "# Mamba2 Model Definitions\n",
        "@dataclass\n",
        "class MambaConfig:\n",
        "    d_model: int = 768\n",
        "    d_intermediate: int = 0\n",
        "    n_layer: int = 24\n",
        "    vocab_size: int = 50277\n",
        "    ssm_cfg: dict = None\n",
        "    attn_layer_idx: list = None\n",
        "    attn_cfg: dict = None\n",
        "    rms_norm: bool = True\n",
        "    residual_in_fp32: bool = True\n",
        "    fused_add_norm: bool = True\n",
        "    pad_vocab_size_multiple: int = 16\n",
        "    tie_embeddings: bool = True\n",
        "\n",
        "    def __post_init__(self):\n",
        "        # 기본값으로 설정된 None 타입을 빈 딕셔너리와 빈 리스트로 초기화\n",
        "        if self.ssm_cfg is None:\n",
        "            self.ssm_cfg = {\"layer\": \"Mamba2\"}\n",
        "        if self.attn_layer_idx is None:\n",
        "            self.attn_layer_idx = []\n",
        "        if self.attn_cfg is None:\n",
        "            self.attn_cfg = {}\n",
        "\n",
        "    def to_json_string(self):\n",
        "        return json.dumps(asdict(self))\n",
        "\n",
        "    def to_dict(self):\n",
        "        return asdict(self)\n",
        "\n",
        "\n",
        "classes = ['Algorithms', 'Backend', 'Data Science', 'Databases', 'Dev Tools', 'Frontend', 'Mobile', 'Systems', 'iOS/macOS']\n",
        "class2id = {'Algorithms' :0, 'Backend' : 1, 'Data Science' : 2, 'Databases' : 3, 'Dev Tools' : 4, 'Frontend' : 5, 'Mobile' :6, 'Systems' : 7, 'iOS/macOS' : 8}\n",
        "id2class = {0 : 'Algorithms', 1: 'Backend', 2 : 'Data Science', 3 : 'Databases', 4 : 'Dev Tools', 5 : 'Frontend', 6 : 'Mobile', 7 : 'Systems', 8 :'iOS/macOS'}\n",
        "\n",
        "\n",
        "class MambaClassificationHead(nn.Module):\n",
        "    def __init__(self, d_model, num_classes, **kwargs):\n",
        "        super(MambaClassificationHead, self).__init__()\n",
        "        self.classification_head = nn.Linear(d_model, num_classes, **kwargs)\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        return self.classification_head(hidden_states)\n",
        "\n",
        "class MambaTextClassification(MambaLMHeadModel):\n",
        "    def __init__(\n",
        "        self,\n",
        "        config: MambaConfig,\n",
        "        initializer_cfg=None,\n",
        "        device=None,\n",
        "        dtype=None,\n",
        "    ) -> None:\n",
        "        super().__init__(config, initializer_cfg, device, dtype)\n",
        "\n",
        "        self.classification_head = MambaClassificationHead(d_model=config.d_model, num_classes=len(classes))\n",
        "        del self.lm_head\n",
        "        self.multi_label = True\n",
        "        self.id2label = id2class\n",
        "        self.class2id = class2id\n",
        "\n",
        "    @classmethod\n",
        "    def addMambaClassificationHead(cls, num_classes, id2label, class2id, multi_label):\n",
        "        cls.classification_head = MambaClassificationHead\n",
        "        del self.lm_head\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
        "        # Backbone을 통해 hidden_states 얻기 (이 부분은 실제 백본 모델로 대체되어야 함)\n",
        "        hidden_states = self.backbone(input_ids)  # 가상의 backbone 사용\n",
        "        mean_hidden_states = hidden_states.mean(dim=1)\n",
        "\n",
        "        logits = self.classification_head(mean_hidden_states)\n",
        "        if labels is None:\n",
        "            ClassificationOutput = namedtuple(\"ClassificationOutput\", [\"logits\"])\n",
        "            return ClassificationOutput(logits=logits)\n",
        "        else:\n",
        "            ClassificationOutput = namedtuple(\"ClassificationOutput\", [\"loss\", \"logits\"])\n",
        "            loss_fct = nn.BCEWithLogitsLoss() if self.multi_label else nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits, labels)\n",
        "            return ClassificationOutput(loss=loss, logits=logits)\n",
        "\n",
        "    def predict(self, text, tokenizer):\n",
        "        input_ids = torch.tensor(tokenizer(text)['input_ids'], device='cuda')[None]\n",
        "        with torch.no_grad():\n",
        "          logits = self.forward(input_ids).logits[0]\n",
        "\n",
        "        if self.multi_label:\n",
        "          probabilities = torch.sigmoid(logits).cpu().numpy()\n",
        "          predictions = (probabilities > 0.5).astype(int)\n",
        "          return [self.id2label[i] for i, value in enumerate(predictions) if value == 1]\n",
        "        else:\n",
        "          label = np.argmax(logits.cpu().numpy())\n",
        "          return self.id2label[label]\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, pretrained_model_name, device=None, dtype=None):\n",
        "        config_data = load_config_hf(pretrained_model_name)  # 사전 훈련된 설정 로드\n",
        "        config = MambaConfig(**config_data)\n",
        "        model = cls(config)\n",
        "        model_state_dict = load_state_dict_hf(pretrained_model_name, device=device, dtype=dtype)\n",
        "        model.load_state_dict(model_state_dict, strict=False)\n",
        "        return model\n",
        "\n",
        "###########\n",
        "\n",
        "\n",
        "# 모델 로드 함수 (Hugging Face와 로컬 모델을 지원)\n",
        "@st.cache_resource\n",
        "def load_classifier(model_path, is_local=False, model_type='default'):\n",
        "    try:\n",
        "        if model_type == 'llmembed':\n",
        "\n",
        "            llama2_tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\", use_auth_token=\"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\", trust_remote_code=True)\n",
        "            llama2_tokenizer.pad_token = llama2_tokenizer.eos_token  # 패딩 토큰 설정\n",
        "            llama2_config = AutoConfig.from_pretrained(\"meta-llama/Llama-3.2-1B\",use_auth_token=\"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\", output_hidden_states=True)\n",
        "            llama2_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\",use_auth_token=\"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\", config=llama2_config)\n",
        "\n",
        "            bert_tokenizer = BertTokenizer.from_pretrained('google-bert/bert-large-uncased')\n",
        "            bert_model = BertModel.from_pretrained('google-bert/bert-large-uncased')\n",
        "\n",
        "            roberta_tokenizer = RobertaTokenizer.from_pretrained('FacebookAI/roberta-large')\n",
        "            roberta_model = RobertaModel.from_pretrained('FacebookAI/roberta-large')\n",
        "\n",
        "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "            llama2_model.eval().to(device)\n",
        "            bert_model.eval().to(device)\n",
        "            roberta_model.eval().to(device)\n",
        "\n",
        "            downstream_model = DownstreamModel(class_num=5, SIGMA=0.1).to(device)\n",
        "\n",
        "            model_load_path = \"/content/drive/MyDrive/Streamlit/model_weights_stackexchange_llama3_2.pth\"\n",
        "            downstream_model.load_state_dict(torch.load(model_load_path, map_location=device))\n",
        "            downstream_model.eval()\n",
        "\n",
        "            st.success(f\"LLMEmbed 모델 로드 성공\")\n",
        "            return {\n",
        "                'llama2_tokenizer': llama2_tokenizer,\n",
        "                'llama2_model': llama2_model,\n",
        "                'bert_tokenizer': bert_tokenizer,\n",
        "                'bert_model': bert_model,\n",
        "                'roberta_tokenizer': roberta_tokenizer,\n",
        "                'roberta_model': roberta_model,\n",
        "                'downstream_model': downstream_model,\n",
        "                'device': device\n",
        "            }\n",
        "        elif model_type == 'mamba2':\n",
        "            config = MambaConfig  # 필요한 경우 여기에 구성 파라미터를 추가\n",
        "            model = MambaTextClassification.from_pretrained(model_path)\n",
        "            tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "            model.to(device)  # 모델을 디바이스로 이동\n",
        "            tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "            st.success(\"Mamba2 모델 로드 성공\")\n",
        "            return {'tokenizer': tokenizer, 'model': model}\n",
        "        else:\n",
        "            if is_local:\n",
        "                tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n",
        "                model = AutoModelForSequenceClassification.from_pretrained(model_path, local_files_only=True)\n",
        "                st.success(f\"모델 로드 성공\")\n",
        "            else:\n",
        "                tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "                model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "                st.success(f\"모델 로드 성공\")\n",
        "            return {'tokenizer': tokenizer, 'model': model}\n",
        "    except Exception as e:\n",
        "        st.error(f\"모델 또는 토크나이저 로딩 중 오류 발생: {e}\")\n",
        "\n",
        "# def load_mamba2_model(device):\n",
        "#     model = MambaTextClassification.from_pretrained(\"ebinna/multi_cls_mamba2-130m_redo\", device=device)\n",
        "#     tokenizer = AutoTokenizer.from_pretrained(\"ebinna/multi_cls_mamba2-130m_redo\")\n",
        "#     tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "#     return model, tokenizer\n",
        "\n",
        "def classify_text(text, tokenizer, model, labels, multi_label=False):\n",
        "    # 입력 텍스트를 토크나이저로 처리하고 텐서로 변환\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "    device = next(model.parameters()).device  # 모델이 위치한 장치 가져오기\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}  # 모든 입력을 모델 장치로 이동\n",
        "\n",
        "    # 모델에 입력을 전달하여 출력 logits 계산\n",
        "    outputs = model(**inputs)\n",
        "    logits = outputs.logits\n",
        "\n",
        "    # 다중 레이블 분류인지 여부에 따라 로직 분기\n",
        "    if multi_label:\n",
        "        scores = torch.sigmoid(logits).squeeze().tolist()\n",
        "    else:\n",
        "        scores = torch.softmax(logits, dim=1).squeeze().tolist()\n",
        "\n",
        "    # 레이블과 점수를 매핑하여 정렬된 결과 반환\n",
        "    result = sorted(zip(labels, scores), key=lambda x: x[1], reverse=True)\n",
        "    return result\n",
        "\n",
        "# def classify_text(text, tokenizer, model, labels, multi_label=False):\n",
        "#     # 입력 텍스트를 토크나이저로 처리하고 텐서로 변환\n",
        "#     inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "#     device = next(model.parameters()).device  # 모델이 위치한 장치 가져오기\n",
        "#     inputs = {k: v.to(device) for k, v in inputs.items()}  # 모든 입력을 모델 장치로 이동\n",
        "\n",
        "#     # 모델에 입력을 전달하여 출력 logits 계산\n",
        "#     outputs = model(**inputs)\n",
        "#     logits = outputs.logits\n",
        "\n",
        "#     # 레이블과 점수를 매핑하여 정렬된 결과 반환\n",
        "#     result = sorted(zip(labels, logits.squeeze().tolist()), key=lambda x: x[1], reverse=True)\n",
        "#     return result\n",
        "\n",
        "# LLMEmbed 모델 추론 함수\n",
        "def infer_llmembed(text, model_components):\n",
        "    device = model_components['device']\n",
        "    llama2_tokenizer = model_components['llama2_tokenizer']\n",
        "    llama2_model = model_components['llama2_model']\n",
        "    bert_tokenizer = model_components['bert_tokenizer']\n",
        "    bert_model = model_components['bert_model']\n",
        "    roberta_tokenizer = model_components['roberta_tokenizer']\n",
        "    roberta_model = model_components['roberta_model']\n",
        "    downstream_model = model_components['downstream_model']\n",
        "\n",
        "    # 각 모델로부터 임베딩을 추출\n",
        "    llama2_emb = get_llama2_embedding(text, llama2_tokenizer, llama2_model, device)\n",
        "    bert_emb = get_bert_embedding(text, bert_tokenizer, bert_model, device)\n",
        "    roberta_emb = get_roberta_embedding(text, roberta_tokenizer, roberta_model, device)\n",
        "\n",
        "    # Forward pass through the downstream model\n",
        "    with torch.no_grad():\n",
        "        prediction = downstream_model(llama2_emb, bert_emb, roberta_emb)\n",
        "        scores = prediction.squeeze().tolist()\n",
        "        result = sorted(zip(label_map.values(), scores), key=lambda x: x[1], reverse=True)\n",
        "    return result\n",
        "\n",
        "def get_llama2_embedding(text, tokenizer, model, device):\n",
        "    inputs = tokenizer(text, return_tensors='pt', max_length=128, padding=\"max_length\", truncation=True).to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        # Average the last 5 layers\n",
        "        embedding = torch.stack([torch.mean(outputs.hidden_states[i], dim=1) for i in range(-1, -6, -1)], dim=1)\n",
        "        # outputs = model(**inputs, output_hidden_states=True)\n",
        "        # # 마지막 5개 레이어의 평균을 사용\n",
        "        # hidden_states = outputs.hidden_states[-5:]\n",
        "        # embedding = torch.mean(torch.stack(hidden_states), dim=0).mean(dim=1)\n",
        "    return embedding\n",
        "\n",
        "def get_bert_embedding(text, tokenizer, model, device):\n",
        "    inputs = tokenizer(text, return_tensors='pt', max_length=128, padding=\"max_length\", truncation=True).to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        embedding = outputs.pooler_output\n",
        "    return embedding\n",
        "\n",
        "def get_roberta_embedding(text, tokenizer, model, device):\n",
        "    inputs = tokenizer(text, return_tensors='pt', max_length=128, padding=\"max_length\", truncation=True).to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        embedding = outputs.last_hidden_state[:, 0, :]\n",
        "    return embedding\n",
        "\n",
        "def main():\n",
        "\n",
        "    st.markdown(\"\"\"\n",
        "        <style>\n",
        "            .main-bg {\n",
        "                background-color: #ffffff;  /* 완전한 흰색 배경 */\n",
        "                padding: 30px;\n",
        "                border-radius: 10px;\n",
        "                box-shadow: 0px 4px 8px rgba(0, 0, 0, 0.1);\n",
        "            }\n",
        "        </style>\n",
        "    \"\"\", unsafe_allow_html=True)\n",
        "\n",
        "    st.markdown(\"\"\"\n",
        "        <style>\n",
        "            .main-bg {\n",
        "                background-color: #f5f5f5;  /* 밝은 회색 배경 */\n",
        "                padding: 30px;\n",
        "                border-radius: 10px;\n",
        "                box-shadow: 0px 4px 8px rgba(0, 0, 0, 0.2);\n",
        "            }\n",
        "            .title {\n",
        "                font-size: 2.5em;\n",
        "                color: #2A75AF;\n",
        "                font-weight: bold;\n",
        "                text-align: center;\n",
        "                margin-bottom: 0.2em;\n",
        "            }\n",
        "            .subtitle {\n",
        "                font-size: 1.2em;\n",
        "                color: #444;\n",
        "                text-align: center;\n",
        "                margin-top: 0;\n",
        "                margin-bottom: 1.5em;\n",
        "            }\n",
        "            .description {\n",
        "                background-color: #e6f7f9;\n",
        "                padding: 20px;\n",
        "                border-radius: 8px;\n",
        "                box-shadow: 2px 2px 10px rgba(0, 0, 0, 0.1);\n",
        "                font-size: 1em;\n",
        "                color: #333;\n",
        "                margin-bottom: 1.5em;\n",
        "            }\n",
        "            div.stButton > button:first-child {\n",
        "              background-color: #4CAF50; /* 초록색 배경 */\n",
        "              color: white;\n",
        "              border: None;\n",
        "              border-radius: 5px;\n",
        "              height: 45px;\n",
        "              width: 100px;\n",
        "              font-size: 16px;\n",
        "              transition: background-color 0.3s ease;\n",
        "            }\n",
        "            div.stButton > button:hover {\n",
        "                background-color: #45a049;\n",
        "            }\n",
        "        </style>\n",
        "\n",
        "        <div class=\"title\">SYNexis: Text Classification Platform</div>\n",
        "        <div class=\"subtitle\">A Collaborative Innovation by Samsung SDS and Yonsei University</div>\n",
        "\n",
        "        <div class=\"description\">\n",
        "            <p><b>SYNexis</b> harnesses advanced machine learning to classify text inputs with accuracy and insight, delivering smart solutions for complex data interpretation.</p>\n",
        "            <p>Developed through a unique partnership between <b>Samsung SDS</b> and <b>Yonsei University</b>, SYNexis stands at the forefront of intelligent systems.</p>\n",
        "            <p>Explore the power of AI and experience the future of data classification.</p>\n",
        "        </div>\n",
        "    \"\"\", unsafe_allow_html=True)\n",
        "\n",
        "    st.write(\"Enter your message below to receive customized AI-driven classification results:\")\n",
        "\n",
        "    # 설정 확장 탭\n",
        "    with st.expander(\"🔧 Settings\"):\n",
        "        st.write(\"Choose models and start classification.\")\n",
        "\n",
        "        # 모델 선택 옵션 정의\n",
        "        model_options = {\n",
        "            \"Single Class Sobert\": (\"/content/drive/MyDrive/Streamlit/single_cls_sobert_base\", True, 'default'),\n",
        "            \"Single Class LLMEmbed\": (\"llmembed\", True, 'llmembed'),\n",
        "            \"Multi Label Sobert\": (\"/content/drive/MyDrive/Streamlit/multi_cls_sobert_base\", True, 'default'),\n",
        "            \"Multi Label Mamba2\": (\"ebinna/multi_cls_mamba2-130m\", False, 'mamba2')\n",
        "\n",
        "        }\n",
        "\n",
        "        # 모델 선택 탭 구성\n",
        "        tabs = st.tabs([\"Single-Class Model Selection\", \"Multi-Label Model Selection\"])\n",
        "        with tabs[0]:\n",
        "            st.write(\"**Single-Class Model**\")\n",
        "            multi_class_choice = st.selectbox(\"Choose a Single-Class Classification Model:\", list(model_options.keys()), key=\"multi_class_model\")\n",
        "            multi_class_path, multi_class_is_local, multi_class_type = model_options[multi_class_choice]\n",
        "            multi_class_model_components = load_classifier(multi_class_path, is_local=multi_class_is_local, model_type=multi_class_type)\n",
        "\n",
        "        with tabs[1]:\n",
        "            st.write(\"**Multi-Label Model**\")\n",
        "            multi_label_choice = st.selectbox(\"Choose a Multi-Label Classification Model:\", list(model_options.keys()), key=\"multi_label_model\")\n",
        "            multi_label_path, multi_label_is_local, multi_label_type = model_options[multi_label_choice]\n",
        "            multi_label_model_components = load_classifier(multi_label_path, is_local=multi_label_is_local, model_type=multi_label_type)\n",
        "\n",
        "    post = st.text_area(\"✍️ Compose your text here:\", placeholder=\"Type your message here and press Submit.\")\n",
        "\n",
        "    if st.button(\"Submit\"):\n",
        "        if post:\n",
        "            with st.spinner(\"Classifying...\"):\n",
        "                # 세션 상태 초기화\n",
        "                st.session_state.primary_label = None\n",
        "                st.session_state.primary_score = None\n",
        "                st.session_state.multi_label_labels = []\n",
        "                st.session_state.second_result = []\n",
        "\n",
        "                # Single-Class Classification 수행\n",
        "                if multi_class_choice == \"Single Class LLMEmbed\":\n",
        "                    result = infer_llmembed(post, multi_class_model_components)\n",
        "                else:\n",
        "                    tokenizer = multi_class_model_components['tokenizer']\n",
        "                    model = multi_class_model_components['model']\n",
        "                    result = classify_text(post, tokenizer, model, first_labels)\n",
        "                primary_label = result[0][0]\n",
        "\n",
        "                # 세션 상태 저장\n",
        "                st.session_state.primary_label = primary_label\n",
        "                st.session_state.primary_score = result[0][1]\n",
        "\n",
        "                if primary_label.lower() == \"tech\":\n",
        "                    # Multi-Label Classification 수행\n",
        "                    if multi_label_choice == \"Single Class LLMEmbed\":\n",
        "                        second_result = infer_llmembed(post, multi_label_model_components)\n",
        "                    else:\n",
        "                        tokenizer = multi_label_model_components['tokenizer']\n",
        "                        model = multi_label_model_components['model']\n",
        "                        if multi_label_choice == \"Multi Label Mamba2\":\n",
        "                            second_result = classify_text(post, tokenizer, model, second_labels, multi_label=True)\n",
        "                        else:\n",
        "                            second_result = classify_text(post, tokenizer, model, second_labels, multi_label=True)\n",
        "                    multi_label_labels = [label for label, score in second_result if score >= 0.5]\n",
        "\n",
        "                    st.session_state.multi_label_labels = multi_label_labels\n",
        "                    st.session_state.second_result = second_result\n",
        "\n",
        "    # 결과 출력\n",
        "    st.write(\"---\")\n",
        "    if \"primary_label\" in st.session_state and st.session_state.primary_label:\n",
        "        #st.subheader(\"📌 Classification Results\")\n",
        "        #st.markdown(f\"**1. Single-Class Classification:** :blue[{st.session_state.primary_label.capitalize()}]\")\n",
        "        st.markdown(\"\"\"\n",
        "        <div class=\"main-bg\">\n",
        "            <h2>📌 Classification Results</h2>\n",
        "            <h4 style=\"color:#2A75AF;\"><b>1. Single-Class Classification:</b></h4>\n",
        "            <p><b>Label:</b> <span style=\"color:blue;\">{}</span></p>\n",
        "        </div>\n",
        "    \"\"\".format(st.session_state.primary_label.capitalize()), unsafe_allow_html=True)\n",
        "\n",
        "\n",
        "\n",
        "    if \"multi_label_labels\" in st.session_state and st.session_state.multi_label_labels:\n",
        "        #st.markdown(\"**2. Multi-Label Classification:**\")\n",
        "        st.markdown(\"\"\"\n",
        "        <div class=\"main-bg\">\n",
        "            <h4 style=\"color:#2A75AF;\"><b>2. Multi-Label Classification:</b></h4>\n",
        "            <p>{}</p>\n",
        "        </div>\n",
        "    \"\"\".format(\", \".join([f\"<span style='color:green;'>{label}</span>\" for label in st.session_state.multi_label_labels])), unsafe_allow_html=True)\n",
        "\n",
        "        #st.write(\", \".join([f\":green[{label}]\" for label in st.session_state.multi_label_labels]))\n",
        "\n",
        "    # Score 확인 옵션 제공\n",
        "    if \"primary_label\" in st.session_state and st.session_state.primary_label and st.checkbox(\"Show Scores\"):\n",
        "        st.subheader(\"📊 Score\")\n",
        "        st.markdown(\"**Single-Class Classification Score:**\")\n",
        "        st.write(f\"{st.session_state.primary_label.capitalize()}: {st.session_state.primary_score:.4f}\")\n",
        "\n",
        "        st.markdown(\"**Multi-Label Classification Scores:**\")\n",
        "        for label, score in st.session_state.second_result:\n",
        "            st.write(f\"{label}: {score:.4f}\")\n",
        "\n",
        "# 레이블 맵\n",
        "label_map = {0: \"biology\", 1: \"cooking\", 2: \"diy\", 3: \"travel\", 4: \"tech\"}\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # 분류 레이블 설정\n",
        "    first_labels = ['biology', 'cooking', 'diy', 'travel', 'tech']\n",
        "    second_labels = ['Algorithms', 'Backend', 'Data Science', 'Databases', 'Dev Tools', 'Frontend', 'Mobile', 'Systems', 'iOS/macOS']\n",
        "    main()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZzK9uYZhObn",
        "outputId": "0ce12e53-2cc0-4444-ede7-f6a209c1bfd5"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok authtoken 2oliTZz4PHkBzzjyJqxeTekoth5_4zbCCm3wJRnNrZaMn98FL"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HMj63Ti2fzd4",
        "outputId": "89503c0c-accf-4f46-e595-c6c9da29c3e1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "ngrok.kill()\n",
        "\n",
        "public_url = ngrok.connect(8501)\n",
        "print('공개 URL:', public_url.public_url)\n",
        "\n",
        "# Streamlit 앱 실행\n",
        "!streamlit run app.py &>/dev/null&\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3s7CUhDcgfWT",
        "outputId": "a7be4481-9de4-4154-b404-3e4a4fe7fae8"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "공개 URL: https://e79f-34-46-173-119.ngrok-free.app\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# 장치 설정\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# DownstreamModel 정의\n",
        "class DownstreamModel(nn.Module):\n",
        "    def __init__(self, class_num, SIGMA):\n",
        "        super(DownstreamModel, self).__init__()\n",
        "        self.SIGMA = SIGMA\n",
        "        self.compress_layers = nn.ModuleList([nn.Sequential(\n",
        "            nn.Linear(2048, 1024), nn.ReLU(), nn.Dropout(0.5)) for _ in range(5)])\n",
        "\n",
        "        self.fc1 = nn.Linear(2097, 1024)\n",
        "        self.fc2 = nn.Linear(1024, 256)\n",
        "        self.fc3 = nn.Linear(256, class_num)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, input_l, input_b, input_r):\n",
        "        batch_size = input_l.shape[0]\n",
        "        split_tensors = torch.split(input_l, 1, dim=1)\n",
        "        input = [self.compress_layers[i](split_tensor.reshape(batch_size, -1)) for i, split_tensor in enumerate(split_tensors)]\n",
        "        input.append(input_b)\n",
        "        input.append(input_r)\n",
        "        input = torch.stack(input, dim=1)\n",
        "        input_T = input.transpose(1, 2)\n",
        "        input_P = torch.matmul(input, input_T)\n",
        "        input_P = input_P.reshape(batch_size, -1)\n",
        "        input_P = 2 * F.sigmoid(self.SIGMA * input_P) - 1\n",
        "\n",
        "        a = torch.mean(input_l, dim=1)\n",
        "        input = torch.cat([input_P, a], dim=1)\n",
        "        output = self.fc1(input)\n",
        "        output = self.fc2(output)\n",
        "        output = self.fc3(output)\n",
        "        output = self.softmax(output)\n",
        "        return output\n",
        "\n",
        "# 모델 초기화\n",
        "downstream_model = DownstreamModel(class_num=5, SIGMA=0.1).to(device)\n",
        "\n",
        "# 가중치 로드 경로 설정\n",
        "model_load_path = \"/content/drive/MyDrive/Streamlit/model_weights_stackexchange_llama3_2.pth\"\n",
        "\n",
        "# 가중치 로드 및 차원 출력\n",
        "try:\n",
        "    state_dict = torch.load(model_load_path, map_location=device)\n",
        "    downstream_model.load_state_dict(state_dict)\n",
        "    print(\"Model weights loaded successfully.\")\n",
        "except RuntimeError as e:\n",
        "    print(f\"Error loading weights: {e}\")\n",
        "\n",
        "# 각 레이어의 가중치 크기 확인\n",
        "print(\"\\nModel Weights and Dimensions:\")\n",
        "for name, param in downstream_model.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        print(f\"Layer: {name} | Size: {param.size()} | Loaded Weight Size: {state_dict[name].size()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-QOl33C94uM",
        "outputId": "d9c2c762-9f8b-47bd-877a-95c256f6bd9a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model weights loaded successfully.\n",
            "\n",
            "Model Weights and Dimensions:\n",
            "Layer: compress_layers.0.0.weight | Size: torch.Size([1024, 2048]) | Loaded Weight Size: torch.Size([1024, 2048])\n",
            "Layer: compress_layers.0.0.bias | Size: torch.Size([1024]) | Loaded Weight Size: torch.Size([1024])\n",
            "Layer: compress_layers.1.0.weight | Size: torch.Size([1024, 2048]) | Loaded Weight Size: torch.Size([1024, 2048])\n",
            "Layer: compress_layers.1.0.bias | Size: torch.Size([1024]) | Loaded Weight Size: torch.Size([1024])\n",
            "Layer: compress_layers.2.0.weight | Size: torch.Size([1024, 2048]) | Loaded Weight Size: torch.Size([1024, 2048])\n",
            "Layer: compress_layers.2.0.bias | Size: torch.Size([1024]) | Loaded Weight Size: torch.Size([1024])\n",
            "Layer: compress_layers.3.0.weight | Size: torch.Size([1024, 2048]) | Loaded Weight Size: torch.Size([1024, 2048])\n",
            "Layer: compress_layers.3.0.bias | Size: torch.Size([1024]) | Loaded Weight Size: torch.Size([1024])\n",
            "Layer: compress_layers.4.0.weight | Size: torch.Size([1024, 2048]) | Loaded Weight Size: torch.Size([1024, 2048])\n",
            "Layer: compress_layers.4.0.bias | Size: torch.Size([1024]) | Loaded Weight Size: torch.Size([1024])\n",
            "Layer: fc1.weight | Size: torch.Size([1024, 2097]) | Loaded Weight Size: torch.Size([1024, 2097])\n",
            "Layer: fc1.bias | Size: torch.Size([1024]) | Loaded Weight Size: torch.Size([1024])\n",
            "Layer: fc2.weight | Size: torch.Size([256, 1024]) | Loaded Weight Size: torch.Size([256, 1024])\n",
            "Layer: fc2.bias | Size: torch.Size([256]) | Loaded Weight Size: torch.Size([256])\n",
            "Layer: fc3.weight | Size: torch.Size([5, 256]) | Loaded Weight Size: torch.Size([5, 256])\n",
            "Layer: fc3.bias | Size: torch.Size([5]) | Loaded Weight Size: torch.Size([5])\n"
          ]
        }
      ]
    }
  ]
}