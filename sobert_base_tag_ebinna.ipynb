{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyMzAKT3VHUesqiuw+x57x0X",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jeremy-su1/ai-algorithm/blob/main/sobert_base_tag_ebinna.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HjmfLNaI_Tsw"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install datasets>=2.18.0 transformers>=4.38.2 sentence-transformers>=2.5.1 setfit>=1.0.3 accelerate>=0.27.2 seqeval>=1.2.2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TnHOxVeN_1OV",
        "outputId": "5f0d42e2-5d75-471b-f15a-de156f2bc24c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset_train = load_dataset(\n",
        "    'csv',\n",
        "    data_files='/content/drive/My Drive/AiExpertCource/project/dataset/rev_tag_training_samples.csv',\n",
        "    split='train'\n",
        ")\n",
        "dataset_valid = load_dataset(\n",
        "    'csv',\n",
        "    data_files='/content/drive/My Drive/AiExpertCource/project/dataset/rev_tag_validation_samples.csv',\n",
        "    split='train'\n",
        ")"
      ],
      "metadata": {
        "id": "bqymN580ACHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes = ['Algorithms', 'Backend', 'Data Science', 'Databases', 'Dev Tools', 'Frontend', 'Mobile', 'Systems', 'iOS/macOS']\n",
        "class2id = {'Algorithms' :0, 'Backend' : 1, 'Data Science' : 2, 'Databases' : 3, 'Dev Tools' : 4, 'Frontend' : 5, 'Mobile' :6, 'Systems' : 7, 'iOS/macOS' : 8}\n",
        "id2class = {0 : 'Algorithms', 1: 'Backend', 2 : 'Data Science', 3 : 'Databases', 4 : 'Dev Tools', 5 : 'Frontend', 6 : 'Mobile', 7 : 'Systems', 8 :'iOS/macOS'}"
      ],
      "metadata": {
        "id": "DVvqrjUrDnJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_path = 'mmukh/SOBertBase'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqlErohjE2hV",
        "outputId": "1278b2d7-f0aa-41cb-d39a-5b810409faf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "\n",
        "def preprocess_function(example):\n",
        "   all_labels =  ast.literal_eval(example['Tags_new'])\n",
        "   labels = [0. for i in range(len(classes))]\n",
        "   for label in all_labels:\n",
        "       label_id = class2id[label]\n",
        "       labels[label_id] = 1.\n",
        "\n",
        "   example = tokenizer(example['Title'] + ' ' + example['Body'], truncation=True, return_tensors=\"pt\")\n",
        "   example['labels'] = labels\n",
        "   example['input_ids'] = example['input_ids'].squeeze(0)\n",
        "   example['token_type_ids'] = example['token_type_ids'].squeeze(0)\n",
        "   example['attention_mask'] = example['attention_mask'].squeeze(0)\n",
        "   return example\n",
        "\n",
        "\n",
        "tokenized_train_dataset = dataset_train.map(preprocess_function)\n",
        "tokenized_valid_dataset = dataset_valid.map(preprocess_function)\n",
        "\n",
        "tokenized_train_dataset = tokenized_train_dataset.remove_columns([\"Title\", \"Body\", \"Tags_filtered\", \"Tags_list\", \"Tags_new\", \"Algorithms\", \"Backend\", \"Data Science\", \"Databases\", \"Dev Tools\", \"Frontend\", \"Mobile\", \"Systems\", \"iOS/macOS\"])\n",
        "tokenized_valid_dataset = tokenized_valid_dataset.remove_columns([\"Title\", \"Body\", \"Tags_filtered\", \"Tags_list\", \"Tags_new\", \"Algorithms\", \"Backend\", \"Data Science\", \"Databases\", \"Dev Tools\", \"Frontend\", \"Mobile\", \"Systems\", \"iOS/macOS\"])\n"
      ],
      "metadata": {
        "id": "yoLrBqhFE5Ll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "NCMbe_lNGxbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, precision_recall_fscore_support\n",
        "import numpy as np\n",
        "\n",
        "def sigmoid(x):\n",
        "   return 1/(1 + np.exp(-x))\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "\n",
        "  predictions, labels = eval_pred\n",
        "  predictions = sigmoid(predictions)\n",
        "  predictions = (predictions > 0.5).astype(int)\n",
        "  accuracy = accuracy_score(labels, predictions)\n",
        "  precision, recall, f1_score_result, _ = precision_recall_fscore_support(labels, predictions, average='micro')\n",
        "\n",
        "  flat_predictions = predictions.reshape(-1)\n",
        "  flat_labels = labels.reshape(-1)\n",
        "  flat_accuracy = accuracy_score(flat_labels, flat_predictions)\n",
        "\n",
        "  return {\n",
        "        'flat_accuracy' : flat_accuracy,\n",
        "        'accuracy' : accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1_score_result\n",
        "        }"
      ],
      "metadata": {
        "id": "G0PS-5CjHpfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "   model_path, num_labels=len(classes),\n",
        "   id2label=id2class, label2id=class2id,\n",
        "  problem_type = \"multi_label_classification\")\n",
        "\n",
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1zZPWUyIWpi",
        "outputId": "700c541f-422f-4dce-b029-9f3800a60a88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of MegatronBertForSequenceClassification were not initialized from the model checkpoint at mmukh/SOBertBase and are newly initialized: ['bert.embeddings.token_type_embeddings.weight', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MegatronBertForSequenceClassification(\n",
              "  (bert): MegatronBertModel(\n",
              "    (embeddings): MegatronBertEmbeddings(\n",
              "      (word_embeddings): Embedding(50048, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(2048, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): MegatronBertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x MegatronBertLayer(\n",
              "          (attention): MegatronBertAttention(\n",
              "            (ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (self): MegatronBertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): MegatronBertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (intermediate): MegatronBertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): MegatronBertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    )\n",
              "    (pooler): MegatronBertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=9, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "   \"model\",\n",
        "   learning_rate=2e-5,\n",
        "   per_device_train_batch_size=4,\n",
        "   per_device_eval_batch_size=4,\n",
        "   num_train_epochs=5,\n",
        "   weight_decay=0.01,\n",
        "   evaluation_strategy=\"epoch\",\n",
        "   save_strategy=\"epoch\",\n",
        "   load_best_model_at_end=True,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "\n",
        "   model=model,\n",
        "   args=training_args,\n",
        "   train_dataset=tokenized_train_dataset,\n",
        "   eval_dataset=tokenized_valid_dataset,\n",
        "   tokenizer=tokenizer,\n",
        "   data_collator=data_collator,\n",
        "   compute_metrics=compute_metrics,\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBV_JZ-5IeZm",
        "outputId": "63d347e3-d321-47a9-c008-4381c4eeb343"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "sQV3Dt32Iu8h",
        "outputId": "89359899-702a-4692-c31c-0c365366d8c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='25000' max='25000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [25000/25000 1:35:35, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Flat Accuracy</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1 Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.149600</td>\n",
              "      <td>0.113344</td>\n",
              "      <td>0.960752</td>\n",
              "      <td>0.717692</td>\n",
              "      <td>0.848308</td>\n",
              "      <td>0.863567</td>\n",
              "      <td>0.855869</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.107400</td>\n",
              "      <td>0.113787</td>\n",
              "      <td>0.959282</td>\n",
              "      <td>0.701846</td>\n",
              "      <td>0.821737</td>\n",
              "      <td>0.891690</td>\n",
              "      <td>0.855286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.075400</td>\n",
              "      <td>0.118472</td>\n",
              "      <td>0.958701</td>\n",
              "      <td>0.697077</td>\n",
              "      <td>0.818340</td>\n",
              "      <td>0.891943</td>\n",
              "      <td>0.853558</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.057500</td>\n",
              "      <td>0.127632</td>\n",
              "      <td>0.958479</td>\n",
              "      <td>0.696462</td>\n",
              "      <td>0.817622</td>\n",
              "      <td>0.891056</td>\n",
              "      <td>0.852761</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.043300</td>\n",
              "      <td>0.132513</td>\n",
              "      <td>0.958821</td>\n",
              "      <td>0.699692</td>\n",
              "      <td>0.819974</td>\n",
              "      <td>0.890296</td>\n",
              "      <td>0.853690</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=25000, training_loss=0.10023628662109375, metrics={'train_runtime': 5736.0642, 'train_samples_per_second': 17.434, 'train_steps_per_second': 4.358, 'total_flos': 4.131525019816022e+16, 'train_loss': 0.10023628662109375, 'epoch': 5.0})"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "erEZZpoiOdX2",
        "outputId": "74b38594-5f62-4d58-e80b-cf7724706899"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1625' max='1625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1625/1625 01:46]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_loss': 0.11334449797868729,\n",
              " 'eval_flat_accuracy': 0.9607521367521368,\n",
              " 'eval_accuracy': 0.7176923076923077,\n",
              " 'eval_precision': 0.8483076157292185,\n",
              " 'eval_recall': 0.8635672662781859,\n",
              " 'eval_f1_score': 0.8558694287507846,\n",
              " 'eval_runtime': 106.7768,\n",
              " 'eval_samples_per_second': 60.875,\n",
              " 'eval_steps_per_second': 15.219,\n",
              " 'epoch': 5.0}"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "INhc81QCTvYI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tuned_model_path = '/content/drive/My Drive/AiExpertCource/project/sobert_base_sto_tag'  # Î™®Îç∏Í≥º ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†ÄÍ∞Ä Ï†ÄÏû•Îêú Í≤ΩÎ°ú"
      ],
      "metadata": {
        "id": "tH3tewamU0m7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "zlCC65NUVMqb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(tuned_model_path)\n",
        "tokenizer.save_pretrained(tuned_model_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNSTvmzPTWuE",
        "outputId": "c201a183-7073-4e1d-a031-19d187a18f04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/My Drive/AiExpertCource/project/sobert_base_sto_tag/tokenizer_config.json',\n",
              " '/content/drive/My Drive/AiExpertCource/project/sobert_base_sto_tag/special_tokens_map.json',\n",
              " '/content/drive/My Drive/AiExpertCource/project/sobert_base_sto_tag/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tuned_tokenizer = AutoTokenizer.from_pretrained(tuned_model_path)\n",
        "tuned_model = AutoModelForSequenceClassification.from_pretrained(tuned_model_path)"
      ],
      "metadata": {
        "id": "RZgTFY1RU0Ke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ÏòàÏ∏° Ìï®Ïàò Ï†ïÏùò\n",
        "def predict(texts):\n",
        "    # ÌÖçÏä§Ìä∏Î•º ÌÜ†ÌÅ∞ÌôîÌïòÍ≥† ÌÖêÏÑúÎ°ú Î≥ÄÌôò\n",
        "    inputs = tuned_tokenizer(texts, padding='max_length', truncation=True, max_length=2048, return_tensors='pt')\n",
        "\n",
        "    # Î™®Îç∏ÏùÑ ÏÇ¨Ïö©Ìï¥ ÏòàÏ∏° ÏàòÌñâ\n",
        "    with torch.no_grad():\n",
        "        outputs = tuned_model(**inputs)\n",
        "        logits = outputs.logits\n",
        "\n",
        "    # ÏãúÍ∑∏Î™®Ïù¥ÎìúÎ•º ÏÇ¨Ïö©Ìï¥ ÌôïÎ•†Î°ú Î≥ÄÌôò\n",
        "    probabilities = torch.sigmoid(logits).numpy()\n",
        "\n",
        "    # Í∞Å ÌÅ¥ÎûòÏä§Ïóê ÎåÄÌï¥ thresholdÎ•º 0.5Î°ú ÏÑ§Ï†ïÌïòÏó¨ ÏòàÏ∏°Í∞í(0 ÎòêÎäî 1)ÏúºÎ°ú Î≥ÄÌôò\n",
        "    predictions = (probabilities > 0.5).astype(int)\n",
        "\n",
        "    return predictions, probabilities"
      ],
      "metadata": {
        "id": "Iz8s4cUuVBW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ÏÉòÌîå ÌÖçÏä§Ìä∏ ÏûÖÎ†• Î∞è ÏòàÏ∏° ÏàòÌñâ\n",
        "sample_texts = [\n",
        "  \"FFmpeg hevc rtsp stream decoding with frame loss\" +\n",
        "  \"I need to decode my rtsp stream. When i`m using default HEVC and have some corrupted frames, my screen looks like this: Corrupted frame using HEVC, Grey image. But instead of this, I want to have a corrupted picture with pixels issue, like this: Same corrupted frame, but using HEVC_QSV.\"\n",
        "]\n",
        "\n",
        "# ÏòàÏ∏° ÏàòÌñâ\n",
        "predictions, probabilities = predict(sample_texts)"
      ],
      "metadata": {
        "id": "a2s5Z_fvVWHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "arr = np.array(predictions[0])\n",
        "\n",
        "# Í∞íÏù¥ 1Ïù∏ Ïù∏Îç±Ïä§Ïóê Ìï¥ÎãπÌïòÎäî Îß§Ìïë Í∞ÄÏ†∏Ïò§Í∏∞\n",
        "indices = np.where(arr == 1)[0]\n",
        "indices\n",
        "\n",
        "for idx in indices:\n",
        "    tag = id2class[idx]\n",
        "    print(tag)"
      ],
      "metadata": {
        "id": "0RZvXHY_VdVv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}