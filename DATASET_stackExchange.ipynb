{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1evQ3jpbAJkUhHcugxKTwdinyW12M8dJq",
      "authorship_tag": "ABX9TyOyaqOuArtrE2ZQ7xkUJGAO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jeremy-su1/ai-algorithm/blob/main/DATASET_stackExchange.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1VFDSvjH73Mc",
        "outputId": "b51e6985-b997-4287-fa37-15df3030dad1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dataset saved to /content/drive/MyDrive/LLMEmbed/dataset/stackexchange_train_dataset.csv\n",
            "Validation dataset saved to /content/drive/MyDrive/LLMEmbed/dataset/stackexchange_val_dataset.csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 데이터셋 파일 경로\n",
        "biology_path = '/content/drive/MyDrive/LLMEmbed/biology.csv'\n",
        "cooking_path = '/content/drive/MyDrive/LLMEmbed/cooking.csv'\n",
        "diy_path = '/content/drive/MyDrive/LLMEmbed/diy.csv'\n",
        "travel_path = '/content/drive/MyDrive/LLMEmbed/travel.csv'\n",
        "stackoverflow_path = '/content/drive/MyDrive/LLMEmbed/convert_train.csv'\n",
        "\n",
        "# 데이터셋 로드 및 title과 content 결합\n",
        "biology = pd.read_csv(biology_path)\n",
        "biology['title_content'] = biology['title'] + ' ' + biology['content']\n",
        "\n",
        "cooking = pd.read_csv(cooking_path)\n",
        "cooking['title_content'] = cooking['title'] + ' ' + cooking['content']\n",
        "\n",
        "diy = pd.read_csv(diy_path)\n",
        "diy['title_content'] = diy['title'] + ' ' + diy['content']\n",
        "\n",
        "travel = pd.read_csv(travel_path)\n",
        "travel['title_content'] = travel['title'] + ' ' + travel['content']\n",
        "\n",
        "stackoverflow = pd.read_csv(stackoverflow_path)\n",
        "stackoverflow['title_content'] = stackoverflow['Title'] + ' ' + stackoverflow['Body']\n",
        "\n",
        "# 레이블 할당\n",
        "labels = {\n",
        "    'biology': 0,\n",
        "    'cooking': 1,\n",
        "    'diy': 2,\n",
        "    'travel': 3,\n",
        "    'stackoverflow': 4\n",
        "}\n",
        "\n",
        "# 데이터셋을 나누고 레이블을 할당하는 함수\n",
        "def split_and_label(dataset, label, validation_size=400, train_size=2000):\n",
        "    # 검증 데이터셋 추출\n",
        "    validation_data = dataset.sample(n=min(validation_size, len(dataset)), random_state=1)\n",
        "    # 남은 데이터 중에서 훈련 데이터셋 추출\n",
        "    train_data = dataset.drop(validation_data.index).sample(n=min(train_size, len(dataset) - len(validation_data)), random_state=1)\n",
        "    # 레이블 추가\n",
        "    validation_data['label'] = label\n",
        "    train_data['label'] = label\n",
        "    return train_data[['title_content', 'label']], validation_data[['title_content', 'label']]\n",
        "\n",
        "# 데이터셋을 나누고 레이블을 할당\n",
        "biology_train, biology_val = split_and_label(biology, labels['biology'])\n",
        "cooking_train, cooking_val = split_and_label(cooking, labels['cooking'])\n",
        "diy_train, diy_val = split_and_label(diy, labels['diy'])\n",
        "travel_train, travel_val = split_and_label(travel, labels['travel'])\n",
        "crypto_train, crypto_val = split_and_label(stackoverflow, labels['stackoverflow'])\n",
        "\n",
        "# 훈련 데이터셋과 검증 데이터셋을 각각 하나로 합치기\n",
        "combined_train_dataset = pd.concat([biology_train, cooking_train, crypto_train, diy_train, travel_train], ignore_index=True)\n",
        "combined_val_dataset = pd.concat([biology_val, cooking_val, crypto_val, diy_val, travel_val], ignore_index=True)\n",
        "\n",
        "# 결과 데이터셋을 CSV 파일로 저장하기\n",
        "train_csv_path = '/content/drive/MyDrive/LLMEmbed/dataset/stackexchange_train_dataset.csv'\n",
        "val_csv_path = '/content/drive/MyDrive/LLMEmbed/dataset/stackexchange_val_dataset.csv'\n",
        "combined_train_dataset.to_csv(train_csv_path, index=False)\n",
        "combined_val_dataset.to_csv(val_csv_path, index=False)\n",
        "\n",
        "print(f\"Train dataset saved to {train_csv_path}\")\n",
        "print(f\"Validation dataset saved to {val_csv_path}\")"
      ]
    }
  ]
}